{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aloha Ship and Pack - Mililani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACTIONS_2019 = 'data/records_2019_trimmed.csv'\n",
    "TRANSACTIONS_2020 = 'data/records_2020_trimmed.csv'\n",
    "TRANSACTIONS_2021 = 'data/records_2021_trimmed.csv'\n",
    "YEARS_COVERED = '2019 to 2021'\n",
    "DEPARTMENTS  = 'data/sku_to_departments.csv'\n",
    "RETAIL_CATS  = 'data/retail_categories.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Bing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prefixspan import PrefixSpan\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pyfpgrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/Prep Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSVs into Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csvs():\n",
    "    transactions = pd.read_csv(TRANSACTIONS_2019)\n",
    "    for each in [TRANSACTIONS_2020, TRANSACTIONS_2021]:\n",
    "        t_df = pd.read_csv(each)\n",
    "        transactions = transactions.append(t_df)\n",
    "\n",
    "    transactions.reset_index(drop=True, inplace=True)\n",
    "    sku2department = pd.read_csv(DEPARTMENTS)\n",
    "    \n",
    "    del(t_df)\n",
    "    return transactions, sku2department\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean transactions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(trans_df):\n",
    "    #column names\n",
    "    cols_dict = {\n",
    "        'Payment Method' : 'Payment_Method',\n",
    "        'Department' : 'Item_Name',\n",
    "        'Unit Price' : 'Unit_Price',\n",
    "        'Disc' : 'Discount',\n",
    "        'Ext Price' : 'Extended_Price',\n",
    "        'Sub-Total:' : 'Sub_Total',\n",
    "        'Sales Tax:' : 'Sales_Tax',\n",
    "        'Invoice Total:' : 'Invoice_Total',\n",
    "    }\n",
    "    trans_df.rename(columns=cols_dict, inplace=True)\n",
    "\n",
    "    #remove $ signs and commas\n",
    "    cols_to_strip = ['Unit_Price', 'Extended_Price', 'Sub_Total', 'Invoice_Total']\n",
    "    for col in cols_to_strip:\n",
    "        trans_df[col] = trans_df[col].apply(lambda x: x.replace('$', ''))\n",
    "        trans_df[col] = trans_df[col].apply(lambda x: x.replace(',', ''))\n",
    "\n",
    "    #make (###) into -###\n",
    "    trans_df.reset_index(drop=True, inplace=True)\n",
    "    cols_to_sign = ['Unit_Price', 'Extended_Price', 'Sub_Total', 'Invoice_Total']\n",
    "    for col in cols_to_sign:\n",
    "        idxs_to_change = trans_df[col].str.contains(\"(\", regex=False).to_numpy().nonzero()[0]\n",
    "        for i in idxs_to_change:\n",
    "            cell = \"-\" + trans_df.at[i,col][1:-1]\n",
    "            trans_df.at[i,col] = cell\n",
    "\n",
    "    #change types to float\n",
    "    cols_to_float = ['Unit_Price', 'Extended_Price', 'Sub_Total', 'Sales_Tax', 'Invoice_Total']\n",
    "    for col in cols_to_float:\n",
    "        trans_df[col] = trans_df[col].astype(float)\n",
    "\n",
    "    #combine date and time into a datetime object instead of just strings\n",
    "    trans_df['DateTimeStamp'] = pd.to_datetime(trans_df.Date + \" \" + trans_df.Time)\n",
    "    trans_df['DateStamp'] = pd.to_datetime(trans_df.Date)\n",
    "\n",
    "    #drop original columns so its not confusing\n",
    "    trans_df.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "    \n",
    "    return trans_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dept_records(trans_df):\n",
    "    #change Misc Non Tax into misc-non-tax so it doesnt mess with pattern finding\n",
    "    trans_df.loc[(trans_df.Department == 'MISC NON TAX'), 'Department'] = 'Rewards'\n",
    "\n",
    "    #Address missing SKU2Department Records that leave an NA in Department\n",
    "    missing_idxs = trans_df.Department.isna().to_numpy().nonzero()[0]\n",
    "\n",
    "    for i in missing_idxs:\n",
    "        cell = trans_df.at[i, 'SKU']\n",
    "        if 'FEDEX' in cell or 'OAHU-GND' in cell:\n",
    "            trans_df.at[i, 'Department'] = 'FEDEX'\n",
    "        elif 'USPS' in cell or 'FIRSTCLASS' in cell or 'FirstClass' in cell:\n",
    "            trans_df.at[i, 'Department'] = 'USPS'\n",
    "        elif 'DHL' in cell:\n",
    "            trans_df.at[i, 'Department'] = 'DHL'\n",
    "        elif 'MBDWCLUTCH' in cell or 'MBDVOTIVE' in cell:\n",
    "            trans_df.at[i, 'Department'] = 'RETAIL' \n",
    "        elif 'WWXSVR' in cell:\n",
    "            trans_df.at[i, 'Department'] = 'UPS'\n",
    "\n",
    "    # Single letter items\n",
    "    fl = ['F3.5', 'F5', 'F3', 'F2', 'F4', 'F1', 'F10', \n",
    "            'F6', 'F9', 'F7', 'F8', 'F11', 'F12', 'F13']\n",
    "    ml = ['M2','M3']\n",
    "    pl = ['P4', 'P3', 'P5', 'P12', 'P3.5','P1', 'P2', \n",
    "            'P6', 'P7', 'P9', 'P8', 'P10', 'P11']\n",
    "\n",
    "    trans_df.loc[(trans_df.SKU.isin(fl)), 'Department'] = 'USPS'\n",
    "    trans_df.loc[(trans_df.SKU.isin(ml)), 'Department'] = 'USPS'\n",
    "    trans_df.loc[(trans_df.SKU.isin(pl)), 'Department'] = 'USPS'\n",
    "    trans_df.loc[(trans_df.SKU.isin(['MBXR', 'MBX'])), 'Department'] = 'MAILBOX'\n",
    "        \n",
    "    post_missing_idxs = trans_df.Department.isna().to_numpy().nonzero()[0]\n",
    "    print(\"# of Missing department records to get dropped: \", len(post_missing_idxs))\n",
    "\n",
    "    #drop remaining NAs\n",
    "    trans_df.dropna(inplace=True)\n",
    "\n",
    "    return trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean/process data into a dataframe\n",
    "if 'transactions' in locals() or 'transactions' in globals():\n",
    "    print('deleting transactions dataframe')\n",
    "    del transactions\n",
    "\n",
    "retail_cats = pd.read_csv(RETAIL_CATS)\n",
    "transactions, sku2department = load_csvs()\n",
    "transactions = clean_dataframe(transactions)\n",
    "transactions = pd.merge(transactions, sku2department[['SKU', 'Department']], \n",
    "                how='left', left_on='SKU', right_on='SKU')\n",
    "transactions = clean_dept_records(transactions)\n",
    "transactions = pd.merge(transactions, retail_cats, \n",
    "                how='left', left_on='Item_Name', right_on='Item')\n",
    "transactions.rename(columns={'Item_type':'Retail_category'}, inplace=True)\n",
    "\n",
    "print('transactions shape: ', transactions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Load Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe for easier loading\n",
    "today = datetime.today().date()\n",
    "\n",
    "TRANSACTIONS_DF_FILE_PATH = f\"dataframes/transactions_dataframe_{today}.csv\"\n",
    "transactions.to_csv(TRANSACTIONS_DF_FILE_PATH, index=False)\n",
    "print(TRANSACTIONS_DF_FILE_PATH)\n",
    "\n",
    "FIRST_TRANS_FILE_PATH = f\"dataframes/First_Trans_df_{today}.csv\"\n",
    "first_transactions.to_csv(FIRST_TRANS_FILE_PATH, index=False)\n",
    "print(FIRST_TRANS_FILE_PATH)\n",
    "\n",
    "ADDRESS_LIST_FILE_PATH = f\"dataframes/Customer_Locations_{today}.csv\"\n",
    "customer_location_df.to_csv(ADDRESS_LIST_FILE_PATH, index=False)\n",
    "print(ADDRESS_LIST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###load PROCESSED dataframe from csv\n",
    "### Remove if existing\n",
    "\n",
    "for df in ['transactions', 'first_transactions', 'customer_location_df']:\n",
    "    if df in locals() or df in globals():\n",
    "        print(f'deleting {df} dataframe')\n",
    "        del df\n",
    "\n",
    "date = \"2022-03-16\"\n",
    "transactions = pd.read_csv(f'dataframes/transactions_dataframe_{date}.csv')\n",
    "first_transactions = pd.read_csv(f'dataframes/First_Trans_df_{date}.csv')\n",
    "customer_location_df = pd.read_csv(f\"dataframes/Customer_Locations_{date}.csv\")\n",
    "\n",
    "print('Transactions shape:          ', transactions.shape)\n",
    "print('First Transactions shape:   ', first_transactions.shape)\n",
    "print('Customer Locations shape: ', customer_location_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods (Other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write unique items to a csv for further categorization\n",
    "header = \"Item, Item_type\"\n",
    "with open('items.csv', 'w') as f:\n",
    "    f.writelines(header)\n",
    "    for item in transactions[transactions.Department == 'RETAIL'].Item_Name.unique():\n",
    "        line = item + \", \\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for prefixspan CLI\n",
    "def write_first_transactions_to_file(first_trans, file_name='output.txt'):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for line in first_trans:\n",
    "            output_line = \"\"\n",
    "            for element in line:\n",
    "                output_line = output_line + element + ' '\n",
    "            output_line = output_line[:-1] + '\\n'\n",
    "            f.writelines(output_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions by Volume and total sales amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.DataFrame(transactions.Department.value_counts())\n",
    "info.reset_index(inplace=True)\n",
    "info.columns = ['Department', 'Counts']\n",
    "\n",
    "#Departments by sales volume\n",
    "sales_volume = []\n",
    "for dept in transactions.Department.unique():\n",
    "    t_s = transactions[transactions.Department == dept]['Extended_Price']\n",
    "    t_s = int(sum(t_s))\n",
    "    sales_volume.append( (dept,t_s) )\n",
    "\n",
    "sv_df = pd.DataFrame(sales_volume)\n",
    "sv_df.columns = ['Department', 'Total_Sales']\n",
    "\n",
    "counts_total_sales_df = pd.merge(info, sv_df, how='outer', left_on='Department', right_on='Department')\n",
    "\n",
    "#created 'normed' sales volume so it doesnt throw off the graph\n",
    "sales_max = counts_total_sales_df.Total_Sales.max()\n",
    "counts_max = counts_total_sales_df.Counts.max()\n",
    "\n",
    "counts_total_sales_df['normed_total_sales'] = counts_total_sales_df.Total_Sales / sales_max\n",
    "counts_total_sales_df['normed_total_sales'] = (counts_total_sales_df['normed_total_sales'] * counts_max).astype(int)\n",
    "\n",
    "#drop MISC NON TAX as its not really a sales item and its vague\n",
    "idx = counts_total_sales_df[counts_total_sales_df.Department == 'MISC NON TAX'].index\n",
    "counts_total_sales_df.drop(idx, inplace=True)\n",
    "\n",
    "#remove negatives as its throwing off the graph and not very informational\n",
    "counts_total_sales_df.loc[(counts_total_sales_df.normed_total_sales < 0), 'normed_total_sales'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_total_sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Departments by number of transactions \n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize = (12,6))\n",
    "ax = sns.barplot(data = counts_total_sales_df, x='Department', y='Counts', color='dodgerblue')\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation = 30, ha = 'right')\n",
    "ax.set_xlabel('')\n",
    "plt.title(\"By Number of Transactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize = (15,8))\n",
    "ax = sns.barplot(data = counts_total_sales_df, x='Department', y='Total_Sales')\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(),rotation = 30, ha = 'right', size = 14)\n",
    "ax.yaxis.set_major_formatter('${x:,}')\n",
    "# ax.set_yticklabels(ax.get_yticklabels(), size = 14)\n",
    "plt.title(\"By Sales Volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FONT_SIZE = 14\n",
    "TITLE_FONT_SIZE = 16\n",
    "info_df = counts_total_sales_df[['Department', 'Counts', 'normed_total_sales']]\n",
    "info_melt = pd.melt(info_df, id_vars=\"Department\")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(12,8)\n",
    "\n",
    "b = sns.barplot(x='Department', y='value', hue='variable', data=info_melt)\n",
    "\n",
    "###adjust the look of the plot\n",
    "## x-axis\n",
    "x_tick_labels = ax1.xaxis.get_ticklabels()\n",
    "_ = ax1.xaxis.set_ticklabels(x_tick_labels, rotation = 45, ha = 'right', size = BASE_FONT_SIZE-2)\n",
    "_ = ax1.xaxis.set_label_text(\"\")\n",
    "\n",
    "\n",
    "#colorize the x labels\n",
    "shipping_companies = ['USPS', 'UPS', 'FEDEX', 'DHL']\n",
    "for tick in x_tick_labels:\n",
    "    tick_text = tick.get_text()\n",
    "    if tick_text in shipping_companies : tick.set_color('red')\n",
    "\n",
    "## y-axis\n",
    "ax1.yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "_ = ax1.yaxis.set_label_text(\"# of Transactions\")\n",
    "ax1.yaxis.label.set_fontsize(BASE_FONT_SIZE)\n",
    "\n",
    "#right side y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim(ax1.get_ylim())\n",
    "ax2.yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "_ = ax2.set_yticklabels([\"$0\", \"$115,000\", \"$230,000\", \"$345,000\", \"$465,000\"])\n",
    "ax2.set_ylabel('Sales Volume', fontsize=BASE_FONT_SIZE)\n",
    "\n",
    "#adjust the legend\n",
    "ax1.legend(fontsize = TITLE_FONT_SIZE)\n",
    "\n",
    "# legend.set_title('')\n",
    "for t, l in zip(ax1.legend_.texts, ('Transactions', \"Sales Volume\")):\n",
    "    t.set_text(l)\n",
    "\n",
    "# other\n",
    "ax1.tick_params(bottom=True)\n",
    "ax2.tick_params(left=False, right=False)\n",
    "sns.despine(left=True, bottom=False, right=True)\n",
    "fig.suptitle(\"Aloha Ship and Pack\", fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "plt.savefig('figures/trans_by_sales-volume.jpg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historgram of shipping companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "usps_trans  = transactions[(transactions.Department == 'USPS') & (transactions.Extended_Price > 0)]['Extended_Price']\n",
    "fedex_trans = transactions[(transactions.Department == 'FEDEX') & (transactions.Extended_Price > 0)]['Extended_Price']\n",
    "dhl_trans   = transactions[(transactions.Department == 'DHL') & (transactions.Extended_Price > 0)]['Extended_Price']\n",
    "\n",
    "usps_avg  = np.average(usps_trans)\n",
    "fedex_avg = np.average(fedex_trans)\n",
    "dhl_avg   = np.average(dhl_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_C = '1'\n",
    "LINE_W = '1'\n",
    "BIN_WDTH = 30\n",
    "\n",
    "\n",
    "b = sns.histplot()\n",
    "sns.histplot(data = usps_trans, color='brown', alpha=1, bins=20, label='USPS', binwidth=BIN_WDTH)\n",
    "sns.histplot(data = fedex_trans, color='purple', bins=20, label='FedEX', alpha = 0.5, binwidth=BIN_WDTH)\n",
    "sns.histplot(data = dhl_trans, color='yellow', bins=10, label='DHL', alpha = 0.5, binwidth=BIN_WDTH)\n",
    "b.set_yscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_C = '1'\n",
    "LINE_W = '1'\n",
    "BASE_FONT_SIZE = 14\n",
    "BIN_WDTH = 50\n",
    "\n",
    "plt.figure()\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6), sharey=False, sharex=True)\n",
    "ax1, ax2, ax3 = axes\n",
    "\n",
    "#plots\n",
    "sns.histplot(ax=ax1, \n",
    "            data = usps_trans, label='USPS',\n",
    "            color='brown', alpha=1, bins=20,  binwidth=BIN_WDTH)\n",
    "sns.histplot(ax=ax2, \n",
    "            data = fedex_trans, label='FedEX',\n",
    "            color='purple', bins=20, binwidth=BIN_WDTH)\n",
    "sns.histplot(ax=ax3, data = dhl_trans, \n",
    "            color='y', bins=10, label='DHL', binwidth=BIN_WDTH)\n",
    "\n",
    "#xaxis\n",
    "ax1.xaxis.set_label_text('USPS', fontsize=BASE_FONT_SIZE)\n",
    "ax2.xaxis.set_label_text('FedEx', fontsize=BASE_FONT_SIZE)\n",
    "ax3.xaxis.set_label_text('DHL', fontsize=BASE_FONT_SIZE)\n",
    "\n",
    "ax1.xaxis.set_major_formatter('${x:,.0f}')\n",
    "\n",
    "#yaxis\n",
    "ax1.yaxis.set_label_text(\"Number of Transactions\", fontsize=BASE_FONT_SIZE)\n",
    "ax2.yaxis.set_label_text(\"\")\n",
    "ax3.yaxis.set_label_text(\"\")\n",
    "\n",
    "#other\n",
    "sns.set_style('darkgrid')\n",
    "fig.suptitle('Aloha Ship and Pack \\n How many shipping transactions at each price point for each company?', fontsize=16, fontweight='bold')\n",
    "# plt.title(\"thing\")\n",
    "\n",
    "plt.savefig(\"figures/shipping_trans.jpg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do customers buy during their first named transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###load and prep data\n",
    "customers = list(transactions.Customer.unique())\n",
    "customers.pop(customers.index('Cash, Check, Charge'))\n",
    "\n",
    "first_transactions = transactions.sort_values(by=['Customer','DateStamp']).groupby('Customer').head(1)\n",
    "first_transactions = first_transactions[['Customer', 'DateStamp']]\n",
    "first_transactions['transactions'] = None\n",
    "first_transactions['retail_cat'] = None\n",
    "first_transactions.reset_index(drop=True, inplace=True)\n",
    "\n",
    "customers = list(transactions.Customer.unique())\n",
    "print(\"Total Customers = \", len(customers))\n",
    "for i, customer in enumerate(customers):\n",
    "    if i % 250 == 0 : print(str(i) + \"  \", end=\"\")\n",
    "    cust_first_date = first_transactions[first_transactions.Customer == customer]['DateStamp']\n",
    "    cust_first_date = cust_first_date.item()\n",
    "\n",
    "    tdf = transactions.loc[(transactions.Customer == customer) & (transactions.DateStamp == cust_first_date)]\n",
    "    cust_trans = list(tdf['Department'].unique()) #get rid of duplicates [USPS, USPS] but keep as a list\n",
    "    cust_trans = sorted(cust_trans)\n",
    "    cust_idx = first_transactions[first_transactions.Customer == customer].index[0]\n",
    "    first_transactions.at[cust_idx, 'transactions'] = cust_trans\n",
    "\n",
    "    cust_retail = tdf.Retail_category.unique()\n",
    "    first_transactions.at[cust_idx, 'retail_cat'] = cust_retail\n",
    "\n",
    "    # if i > 0 : print(cust_retail)\n",
    "    # if i > 3 : break\n",
    "first_transactions['original_trans'] = first_transactions.transactions\n",
    "first_transactions.transactions = first_transactions.transactions.apply(lambda x: '/'.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prefix Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prefix Span\n",
    "ps = PrefixSpan(first_transactions.transactions)\n",
    "topk = ps.topk(15, closed=False)\n",
    "\n",
    "#doesnt work as desired. Is sequential.  Want frequent patterns as 'sequential' in a single purchase is non-sensical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = first_transactions.transactions\n",
    "patterns = pyfpgrowth.find_frequent_patterns(trans, support_threshold=100)\n",
    "\n",
    "#get the top 15 patterns\n",
    "patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "## make results graphable\n",
    "num_hits, pattern = [], []\n",
    "for each in patterns:\n",
    "    num_hits.append(each[1])\n",
    "    t = '/'.join(each[0])\n",
    "    pattern.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = first_transactions.transactions.value_counts()[0:15]\n",
    "\n",
    "## make results graphable\n",
    "num_hits, pattern = [], []\n",
    "for each in patterns.items():\n",
    "    num_hits.append(each[1])\n",
    "    pattern.append(each[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 15 first Transactions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualizations\n",
    "BASE_FONT_SIZE = 14\n",
    "TITLE_FONT_SIZE = 16\n",
    "sns.set_style('darkgrid')\n",
    "retail = [2,5,9,10,11,12]\n",
    "# rewards = [3,12]\n",
    "\n",
    "colors = []\n",
    "for i in range(len(pattern)):\n",
    "    # if i in rewards:\n",
    "    #     colors.append('green')\n",
    "    if i in retail:\n",
    "        colors.append('blue')\n",
    "    else:\n",
    "        colors.append('black')\n",
    "\n",
    "#create plot\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10,6)\n",
    "b = sns.barplot(x=pattern, y=num_hits, color='#DD3403')\n",
    "\n",
    "#xaxis\n",
    "_ = b.xaxis.set_ticklabels(b.xaxis.get_ticklabels(), rotation = 45, ha = 'right', size = BASE_FONT_SIZE)\n",
    "for i, tick_label in enumerate(b.axes.get_xticklabels()):\n",
    "    tick_label.set_color(colors[i])\n",
    "\n",
    "#yaxis\n",
    "# b.yaxis.set_ticklabels(b.yaxis.get_ticklabels(), size=BASE_FONT_SIZE)\n",
    "# b.yaxis.label.set_fontsize(20)\n",
    "\n",
    "#other\n",
    "b.tick_params(bottom=True)\n",
    "sns.despine(bottom=False, top=True, left=True)\n",
    "\n",
    "#text\n",
    "_ = plt.title('Aloha Ship and Pack \\n Top 15 Most Frequent First Visit Transactions', fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "plt.savefig('figures/first_trans_most_frequent_hq.jpg', bbox_inches='tight', dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 15 first retail transactions type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_retail = first_transactions.retail_cat\n",
    "first_retail = first_retail.explode()\n",
    "first_retail.dropna(inplace=True)\n",
    "\n",
    "first_retail_counts = first_retail.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make results graphable\n",
    "num_hits, pattern = [], []\n",
    "for each in first_retail_counts.items():\n",
    "    num_hits.append(each[1])\n",
    "    pattern.append(each[0])\n",
    "\n",
    "pattern = [p.replace(\"_\", ' ') for p in pattern]\n",
    "\n",
    "###Vizualization\n",
    "BASE_FONT_SIZE = 14\n",
    "TITLE_FONT_SIZE = 16\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "#create plot\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10,6)\n",
    "g = sns.barplot(x=pattern, y=num_hits, color='goldenrod')\n",
    "\n",
    "#xaxis\n",
    "_ = g.xaxis.set_ticklabels(g.xaxis.get_ticklabels(), rotation = 45, ha = 'right', size = BASE_FONT_SIZE)\n",
    "g.bar_label(g.containers[0])\n",
    "\n",
    "#other\n",
    "g.tick_params(bottom=True)\n",
    "sns.despine(bottom=False, top=True, left=True)\n",
    "\n",
    "#titles\n",
    "_ = plt.title(\"Aloha Ship and Pack \\n Retail Categories for First Visit Transactions\", fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "plt.savefig('figures/retail_cats_1st_purchases_hq.jpg', bbox_inches='tight', dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-test on 1st transactions vs overall transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probably would need to comapre the normalized values.  first transactions will by definition be a much smaller size than all transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All transactions vs 1st transactions normalized as xy scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_df = pd.DataFrame(transactions.Department.value_counts(normalize=True))\n",
    "f_df = pd.DataFrame(first_transactions.original_trans.explode().value_counts(normalize=True))\n",
    "info_df = pd.merge(i_df, f_df, left_index=True, right_index=True)\n",
    "info_df.columns = ['All_Trans_Counts', 'First_Trans_Counts']\n",
    "value = info_df.All_Trans_Counts > info_df.First_Trans_Counts\n",
    "info_df['color'] = np.where(value, \"red\", 'plum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_OFFSET = 0.0035\n",
    "Y_OFFSET = -0.003\n",
    "BASE_FONT_SIZE = 14\n",
    "TITLE_FONT_SIZE = 16\n",
    "LABEL_FONT_SIZE = 12\n",
    "items_to_annotate = ['USPS', 'RETAIL', 'COPIES', 'FEDEX', 'NOTARY', 'FAX', 'UPS', 'METER']\n",
    "# sns.set_style('darkgrid')\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8,8)\n",
    "\n",
    "g = sns.scatterplot(data=info_df, x='All_Trans_Counts', y='First_Trans_Counts', hue='color', palette=['purple','red'])\n",
    "\n",
    "g.set_xlim((0, 0.5))\n",
    "g.set_ylim((0, 0.5))\n",
    "g.axline(xy1=(0,0), slope=1, linewidth=0.8, dashes=(4,2))\n",
    "\n",
    "g.xaxis.set_ticklabels('')\n",
    "g.yaxis.set_ticklabels('')\n",
    "\n",
    "#annotations\n",
    "for i, (label, xy) in enumerate(info_df.iterrows()):\n",
    "    # if label in items_to_annotate:\n",
    "\n",
    "    if label in items_to_annotate:\n",
    "        if label == 'FAX':\n",
    "            g.annotate(text=label, xy=(xy[0] + X_OFFSET, xy[1] + Y_OFFSET-0.0025), fontsize=LABEL_FONT_SIZE)\n",
    "        else:\n",
    "            g.annotate(text=label, xy=(xy[0] + X_OFFSET, xy[1] + Y_OFFSET), fontsize=LABEL_FONT_SIZE)\n",
    "\n",
    "g.annotate(\"What Customers Come \\n   the First Time for\", xy=(0.05,0.3), fontsize=BASE_FONT_SIZE, alpha=0.7)\n",
    "g.annotate(\"What Customers \\n Come Back for\", xy=(0.3,0.16), fontsize=BASE_FONT_SIZE, alpha=0.7)\n",
    "#labels and titles\n",
    "g.yaxis.label.set_text(\"First Time Transactions\")\n",
    "g.yaxis.label.set_fontsize(BASE_FONT_SIZE)\n",
    "g.xaxis.label.set_text(\"All Transactions\")\n",
    "g.xaxis.label.set_fontsize(BASE_FONT_SIZE)\n",
    "_ = plt.title(\"Aloha Ship and Pack\", fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "g.legend(\"\")\n",
    "g.legend_.remove()\n",
    "\n",
    "sns.despine(bottom=False, top=True, left=True)\n",
    "\n",
    "plt.savefig(\"figures/all-trans_vs_1st-trans.jpg\", bbox_inches='tight', dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many transactions does each individual customer make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are customers coming from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_list = pd.read_csv('data/address_list_trimmed.csv')\n",
    "address_list.rename(columns={'City/State ' : 'City-State'}, inplace=True)\n",
    "address_list['Full_Address'] = address_list.Address + \", \" + address_list['City-State']\n",
    "\n",
    "address_list.dropna(inplace=True)\n",
    "address_list.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_map_df = pd.DataFrame(transactions.Customer.unique())\n",
    "t_map_df.columns = ['Name']\n",
    "\n",
    "customer_location_df = pd.merge(t_map_df, address_list, how='left', left_on='Name', right_on='Name')\n",
    "del t_map_df\n",
    "del address_list\n",
    "customer_location_df.dropna(inplace=True)\n",
    "customer_location_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup Geolocation encoder\n",
    "BING_API_K = \"AkrYql28S_jhBsb7g_h98xx1BaWVEFexBDp2mlfmjvBjo73vTQPVwnU7rgn6gKT2\"\n",
    "from geopy.geocoders import Bing\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "gb = Bing(api_key=BING_API_K)\n",
    "geo_encoder_BING = RateLimiter(gb.geocode, min_delay_seconds=1)\n",
    "\n",
    "#which chunk of customers to run\n",
    "BEGIN = 4500\n",
    "END   = 4500\n",
    "customer_location_df.loc[BEGIN:, 'location'] = customer_location_df[BEGIN:].Full_Address.progress_apply(geo_encoder_BING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(customer_location_df['location'].isna())\n",
    "# customer_location_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_location_df.loc[::100, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad area below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.Event == 'Dance'),'Event']='Hip-Hop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataframe melt thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Factor': ['Growth', 'Value'],\n",
    "    'Weight': [0.10, 0.20],\n",
    "    'Variance': [0.15, 0.35]\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy = df.melt(id_vars='Factor').rename(columns=str.title)\n",
    "tidy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tick_label in enumerate(g.axes.get_xticklabels()):\n",
    "    print(tick_label.get_text().replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, xy in info_df.iterrows():\n",
    "    g.annotate(text=label, xy=(xy[0], xy[1]))\n",
    "    print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geolocater = Nominatim(user_agent=\"asap\")\n",
    "# from geopy.extra.rate_limiter import RateLimiter\n",
    "# geolocater = RateLimiter(geolocater.geocode, min_delay_seconds=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc47b90a659bbf0cf2e5117b2502bbb016f748103d3a7f4713e21d3bf756069b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('py310')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
